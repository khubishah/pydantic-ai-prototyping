"""
Determinism Stress Test: Ambiguous Prompts
Shows when multi-tool agents become non-deterministic
"""

import asyncio
from pydantic import BaseModel
from pydantic_ai import Agent, RunContext
from typing import Union

# Mock data models
class EntityMetadata(BaseModel):
    name: str
    namespace: str

class CPUMetrics(BaseModel):
    avg_cpu: float
    peak_cpu: float

class LogAnalysis(BaseModel):
    error_count: int
    warning_count: int

# Mock tools
async def get_metadata(ctx: RunContext, entity_name: str) -> dict:
    print(f"üîß TOOL CALLED: get_metadata for {entity_name}")
    return {"name": entity_name, "namespace": "default"}

async def get_cpu_metrics(ctx: RunContext, entity_name: str) -> dict:
    print(f"üîß TOOL CALLED: get_cpu_metrics for {entity_name}")
    return {"avg_cpu": 25.5, "peak_cpu": 45.2}

async def get_log_analysis(ctx: RunContext, entity_name: str) -> dict:
    print(f"üîß TOOL CALLED: get_log_analysis for {entity_name}")
    return {"error_count": 2, "warning_count": 5}

# Specialized Agents
metadata_agent = Agent(
    'openai:gpt-4o-mini',
    output_type=EntityMetadata,
    system_prompt="You retrieve K8s metadata. Always use get_metadata tool."
)

# Multi-tool Agent
unified_agent = Agent(
    'openai:gpt-4o-mini',
    output_type=Union[EntityMetadata, CPUMetrics, LogAnalysis],
    system_prompt="""
    You are a K8s analysis tool. Based on the request, choose the appropriate tool:
    - get_metadata for entity information
    - get_cpu_metrics for CPU analysis  
    - get_log_analysis for log analysis
    
    If the request is ambiguous, make your best guess.
    """
)

# Register tools
metadata_agent.tool(get_metadata)
unified_agent.tool(get_metadata)
unified_agent.tool(get_cpu_metrics) 
unified_agent.tool(get_log_analysis)

async def test_ambiguous_prompts():
    """Test with ambiguous prompts that could match multiple tools"""
    
    print("üß™ TESTING WITH AMBIGUOUS PROMPTS")
    print("=" * 50)
    
    # Ambiguous prompts that could trigger different tools
    ambiguous_prompts = [
        "Analyze pod frontend-123",  # Could be metadata, CPU, or logs
        "Check frontend-123 status",  # Could be metadata or logs
        "Get frontend-123 information",  # Could be any tool
        "Investigate frontend-123",  # Very ambiguous
        "Tell me about frontend-123"  # Very ambiguous
    ]
    
    for prompt in ambiguous_prompts:
        print(f"\nüìù PROMPT: '{prompt}'")
        print("-" * 40)
        
        # Specialized agent - always deterministic
        print("‚úÖ SPECIALIZED AGENT:")
        try:
            result = await metadata_agent.run(prompt)
            print(f"   Result: {type(result.output).__name__}")
        except Exception as e:
            print(f"   ERROR: {e}")
        
        # Multi-tool agent - potentially non-deterministic
        print("‚ùì MULTI-TOOL AGENT:")
        try:
            result = await unified_agent.run(prompt)
            print(f"   Result: {type(result.output).__name__}")
        except Exception as e:
            print(f"   ERROR: {e}")

async def test_multiple_runs():
    """Test same ambiguous prompt multiple times"""
    
    print("\n\nüîÑ TESTING CONSISTENCY ACROSS MULTIPLE RUNS")
    print("=" * 50)
    
    ambiguous_prompt = "Analyze pod frontend-123 performance and status"
    
    print(f"üìù PROMPT: '{ambiguous_prompt}'")
    print("Running 5 times to check consistency...")
    
    specialized_results = []
    unified_results = []
    
    for i in range(5):
        print(f"\n--- Run {i+1} ---")
        
        # Specialized agent
        try:
            result = await metadata_agent.run(ambiguous_prompt)
            specialized_results.append(type(result.output).__name__)
            print(f"Specialized: {type(result.output).__name__}")
        except Exception as e:
            specialized_results.append("ERROR")
            print(f"Specialized: ERROR")
        
        # Multi-tool agent
        try:
            result = await unified_agent.run(ambiguous_prompt)
            unified_results.append(type(result.output).__name__)
            print(f"Multi-tool: {type(result.output).__name__}")
        except Exception as e:
            unified_results.append("ERROR")
            print(f"Multi-tool: ERROR")
    
    print(f"\nüìä CONSISTENCY ANALYSIS:")
    print(f"‚úÖ Specialized Agent Results: {specialized_results}")
    print(f"   Consistency: {len(set(specialized_results)) == 1}")
    print(f"‚ùì Multi-Tool Agent Results: {unified_results}")
    print(f"   Consistency: {len(set(unified_results)) == 1}")
    
    if len(set(specialized_results)) == 1:
        print("‚úÖ Specialized agent is 100% deterministic")
    else:
        print("‚ùå Specialized agent showed inconsistency")
    
    if len(set(unified_results)) == 1:
        print("‚úÖ Multi-tool agent was consistent")
    else:
        print("‚ùå Multi-tool agent showed non-deterministic behavior")

async def main():
    print("üî¨ DETERMINISM STRESS TEST")
    print("=" * 60)
    
    await test_ambiguous_prompts()
    await test_multiple_runs()
    
    print("\n\nüéØ CONCLUSION:")
    print("=" * 30)
    print("‚úÖ Specialized agents are MORE deterministic because:")
    print("   ‚Ä¢ No tool selection ambiguity")
    print("   ‚Ä¢ Focused system prompts")
    print("   ‚Ä¢ Guaranteed output types")
    print("   ‚Ä¢ Predictable behavior")
    print()
    print("‚ùå Multi-tool agents are LESS deterministic because:")
    print("   ‚Ä¢ LLM must choose between multiple tools")
    print("   ‚Ä¢ Ambiguous prompts can trigger different tools")
    print("   ‚Ä¢ Union output types add complexity")
    print("   ‚Ä¢ Context-dependent behavior")

if __name__ == "__main__":
    asyncio.run(main()) 